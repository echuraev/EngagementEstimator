{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "477dcb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef82d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    !pip install mtcnn\n",
    "    !pip install timm==0.4.5\n",
    "    !pip install tensorflow\n",
    "    !pip install numba\n",
    "    #!pip install keras==2.4.3\n",
    "    #!pip install -q torch==1.7.1 torchvision\n",
    "    !pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d53bf9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 08:44:19.404419: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-11 08:44:19.441744: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-11 08:44:19.441768: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-11 08:44:19.441798: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-11 08:44:19.449513: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-11 08:44:20.133601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.1.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mtcnn import MTCNN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential, load_model, model_from_json\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "print(f\"Torch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "397ab23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Activation, Concatenate, Reshape\n",
    "from tensorflow.keras.layers import Flatten, RepeatVector, Permute, TimeDistributed\n",
    "from tensorflow.keras.layers import Multiply, Lambda, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11415773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-PCIE-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Reserved:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Reserved:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e642a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERT2INT=True\n",
    "if CONVERT2INT is True:\n",
    "    N_CLASSES = 2\n",
    "else:\n",
    "    N_CLASSES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f5c11c",
   "metadata": {},
   "source": [
    "#### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12464bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_features(file_name, features):\n",
    "    if os.path.isfile(file_name):\n",
    "        print(\"Error! Cannot save features because file already exists\")\n",
    "        return\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(features, f) # , protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "def load_features(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_weights(model, file_name):\n",
    "    if os.path.isfile(file_name):\n",
    "        print(\"Error! Cannot save features because file already exists\")\n",
    "        return\n",
    "    model.save_weights(file_name)\n",
    "\n",
    "def load_weights(model, file_name):\n",
    "    model.load_weights(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c81de7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "def extract_features(root_dir, directories):\n",
    "    all_features = []\n",
    "    for i, d in enumerate(directories):\n",
    "        if d.startswith('.'):\n",
    "            continue\n",
    "        print('[{}/{}]'.format(i+1, len(directories)), end=\"\\r\") \n",
    "        imgs = []\n",
    "        sequence = []\n",
    "        path = os.path.join(root_dir, d)\n",
    "        for img in os.listdir(path):\n",
    "            if not img.endswith(ext):\n",
    "                continue\n",
    "            img_file = os.path.join(path, img)\n",
    "            if torch_model:\n",
    "                image = Image.open(img_file)\n",
    "                img_tensor = test_transforms(image)\n",
    "                imgs.append(img_tensor)\n",
    "            else:\n",
    "                image = cv2.imread(img_file)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                imgs.append(image)\n",
    "            \n",
    "            if len(imgs) >= BATCH_SIZE:\n",
    "                if torch_model:\n",
    "                    features = cnn_model(torch.stack(imgs, dim=0).to(device))\n",
    "                    features = features.data.cpu().numpy()\n",
    "                else:\n",
    "                    features = cnn_model.predict(np.array(imgs), verbose=0)\n",
    "                if len(sequence) == 0:\n",
    "                    sequence = features\n",
    "                else:\n",
    "                    sequence = np.concatenate((sequence, features),axis=0)\n",
    "                imgs = []\n",
    "                \n",
    "        if len(imgs) > 0:        \n",
    "            if torch_model:\n",
    "                features = cnn_model(torch.stack(imgs, dim=0).to(device))\n",
    "                features = features.data.cpu().numpy()\n",
    "            else:\n",
    "                features = cnn_model.predict(np.array(imgs), verbose=0)\n",
    "            if len(sequence) == 0:\n",
    "                sequence = features\n",
    "            else:\n",
    "                sequence = np.concatenate((sequence, features),axis=0)\n",
    "        \n",
    "        all_features.append(sequence)\n",
    "    return all_features\n",
    "\n",
    "def extract_or_read_features(train_pickle, test_pickle, train_data_files, test_data_files):\n",
    "    if os.path.isfile(train_pickle):\n",
    "        train_features, train_data_files = load_features(train_pickle)\n",
    "    else:\n",
    "        print(\"Create train dataset\")\n",
    "        train_features = extract_features(train_dir, train_data_files)\n",
    "        save_features(train_pickle, [train_features, train_data_files])\n",
    "    if os.path.isfile(test_pickle):\n",
    "        test_features, test_data_files = load_features(test_pickle)\n",
    "    else:\n",
    "        print(\"Create test dataset\")\n",
    "        test_features = extract_features(val_dir, test_data_files)\n",
    "        save_features(test_pickle, [test_features, test_data_files])\n",
    "    return train_features, train_data_files, test_features, test_data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00a77245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file2features(files, features):\n",
    "    res = {}\n",
    "    for i in range(len(files)):\n",
    "        res[files[i]] = features[i]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b37969",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca524362",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='/home/HDD6TB/datasets/emotions/EmotiW/engagement/'\n",
    "ext = \"png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa57818d-5fb6-4935-aa1c-488acf793c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/echuraev/Workspace/HSE/engagement\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae87d3c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/HDD6TB/datasets/emotions/EmotiW/engagement/Engagement_Labels_Engagement.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[1;32m      8\u001b[0m video2label\u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEngagement_Labels_Engagement.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[1;32m     10\u001b[0m     labels_reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(csvfile, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i,row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(labels_reader):\n",
      "File \u001b[0;32m~/anaconda3/envs/sciense/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/HDD6TB/datasets/emotions/EmotiW/engagement/Engagement_Labels_Engagement.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "labels_list = ['distracted', 'engaged']\n",
    "video2label={}\n",
    "float2int={0:0,0.33:1,0.66:2,1:3}\n",
    "float2int={0:0,0.33:0,0.66:1,1:1}\n",
    "\n",
    "import csv\n",
    "video2label={}\n",
    "with open(os.path.join(DATA_DIR,'Engagement_Labels_Engagement.csv'), mode='r') as csvfile:\n",
    "    labels_reader = csv.reader(csvfile, delimiter='\\t')\n",
    "    for i,row in enumerate(labels_reader):\n",
    "        if i==0:\n",
    "            print('first:',row)\n",
    "            continue\n",
    "        videoname,label=row[0],float(row[1])\n",
    "        video2label[videoname]=label\n",
    "        #print(videoname,label)\n",
    "        #if (videoname not in filename2features_val) and (videoname not in filename2features_train):\n",
    "        #    print(videoname,label)\n",
    "#check if fix is incorrect\n",
    "video2label['subject_87_Vid_3']=video2label['subject_77_Vid_6']\n",
    "#video2label = to_categorical(video2label)\n",
    "print(len(video2label))\n",
    "print(video2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4eb5089",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (224, 224)\n",
    "model_urls = {\n",
    "    #'affectnet_7_vggface2_rexnet150.pt': 'https://github.com/HSE-asavchenko/face-emotion-recognition/raw/b55d78c7d2a4c02339de916936fc73749eb58798/models/affectnet_emotions/affectnet_7_vggface2_rexnet150.pt',\n",
    "    # 'enet_b0_7.pt': 'https://github.com/HSE-asavchenko/face-emotion-recognition/raw/b55d78c7d2a4c02339de916936fc73749eb58798/models/affectnet_emotions/enet_b0_7.pt',\n",
    "    'enet_b0_8_best_afew.pt': 'https://github.com/HSE-asavchenko/face-emotion-recognition/raw/b55d78c7d2a4c02339de916936fc73749eb58798/models/affectnet_emotions/enet_b0_8_best_afew.pt',\n",
    "    # 'enet_b0_8_best_vgaf.pt': 'https://github.com/HSE-asavchenko/face-emotion-recognition/raw/b55d78c7d2a4c02339de916936fc73749eb58798/models/affectnet_emotions/enet_b0_8_best_vgaf.pt',\n",
    "    # 'enet_b2_7.pt': 'https://github.com/HSE-asavchenko/face-emotion-recognition/raw/b55d78c7d2a4c02339de916936fc73749eb58798/models/affectnet_emotions/enet_b2_7.pt',\n",
    "    # 'enet_b2_8.pt': 'https://github.com/HSE-asavchenko/face-emotion-recognition/raw/b55d78c7d2a4c02339de916936fc73749eb58798/models/affectnet_emotions/enet_b2_8.pt',\n",
    "    'mobilenet_7.h5': 'https://github.com/HSE-asavchenko/face-emotion-recognition/raw/b55d78c7d2a4c02339de916936fc73749eb58798/models/affectnet_emotions/mobilenet_7.h5',\n",
    "    # 'enet_b0_8_va_mtl.pt': 'https://github.com/HSE-asavchenko/face-emotion-recognition/raw/24c7b22228f88429bf11c64f1f3f292b8e8abe32/models/affectnet_emotions/enet_b0_8_va_mtl.pt',\n",
    "}\n",
    "\n",
    "def download_model(model_url, model_name):\n",
    "    import urllib.request\n",
    "\n",
    "    if not os.path.isfile(model_name):\n",
    "        urllib.request.urlretrieve(model_url, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20c474be",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_DIR = \"features/\"\n",
    "WEIGHTS_DIR = 'weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9a050f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_acc(model,filename2features):\n",
    "    num_correct,num_total=0,0\n",
    "    for fn in filename2features:\n",
    "        features=filename2features[fn]\n",
    "        total_features=None\n",
    "        if USE_ALL_FEATURES:\n",
    "            x=features[0][features[-1]==1]\n",
    "        else:\n",
    "            x=features\n",
    "        max_ind=len(x)-IMAGE_SET_SIZE\n",
    "        if max_ind<=0:\n",
    "            continue\n",
    "        if CONCATENATE_STAT:\n",
    "            stat_x=np.repeat(stat_function(x,axis=0).reshape((1,-1)),len(x),axis=0)\n",
    "            inp = np.expand_dims(np.concatenate((stat_x,x),axis=1), axis=0)\n",
    "            preds=model.predict(inp, verbose=0)[0]\n",
    "        else:\n",
    "            preds=model.predict(np.expand_dims(x, axis=0), verbose=0)[0]\n",
    "        if np.argmax(preds) == float2int[video2label[fn]]:\n",
    "            num_correct+=1\n",
    "        num_total+=1\n",
    "    print(num_total,num_correct/num_total)\n",
    "    \n",
    "def print_acc(model, test_features, test_labels):\n",
    "    num_correct,num_total=0,0\n",
    "    res_preds = []\n",
    "    res_labels = []\n",
    "    for i in range(len(test_features)):\n",
    "        features=test_features[i]\n",
    "        total_features=None\n",
    "        if USE_ALL_FEATURES:\n",
    "            x=features[0][features[-1]==1]\n",
    "        else:\n",
    "            x=features\n",
    "        max_ind=len(x)-IMAGE_SET_SIZE\n",
    "        if max_ind<=0:\n",
    "            continue\n",
    "        if CONCATENATE_STAT:\n",
    "            stat_x=np.repeat(stat_function(x,axis=0).reshape((1,-1)),len(x),axis=0)\n",
    "            inp = np.expand_dims(np.concatenate((stat_x,x),axis=1), axis=0)\n",
    "            preds=model.predict(inp, verbose=0)[0]\n",
    "        else:\n",
    "            preds=model.predict(np.expand_dims(x, axis=0), verbose=0)[0]\n",
    "        res_preds.append(np.argmax(preds))\n",
    "        res_labels.append(test_labels[i])\n",
    "        if res_preds[-1] == test_labels[i]:\n",
    "            num_correct+=1\n",
    "        num_total+=1\n",
    "    acc = num_correct/num_total\n",
    "    print(num_total, acc)\n",
    "    return acc, res_preds, res_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0341c95",
   "metadata": {},
   "source": [
    "### Facial images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f8b44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "FACES_DIR = os.path.join(DATA_DIR, \"frames/faces/mtcnn_aligned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19495122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(FACES_DIR, \"Train\")\n",
    "val_dir = os.path.join(FACES_DIR, \"validation\")\n",
    "train_data_files = os.listdir(train_dir)\n",
    "test_data_files = os.listdir(val_dir)\n",
    "print(len(train_data_files))\n",
    "print(len(test_data_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01946172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "0 40/147: 27.210884353741495%\n",
      "1 107/147: 72.78911564625851%\n",
      "Test:\n",
      "0 14/48: 29.166666666666668%\n",
      "1 34/48: 70.83333333333333%\n"
     ]
    }
   ],
   "source": [
    "def print_classes(files):\n",
    "    labels = []\n",
    "    for f in files:\n",
    "        labels.append(float2int[video2label[f]])\n",
    "    min_num = len(files)\n",
    "    for i in range(N_CLASSES):\n",
    "        count = labels.count(i)\n",
    "        if count < min_num:\n",
    "            min_num = count\n",
    "        percent = count * 100 / len(labels)\n",
    "        print(\"{} {}/{}: {}%\".format(i, count, len(labels), percent))\n",
    "    return min_num\n",
    "\n",
    "print(\"Train:\")\n",
    "train_min_num = print_classes(train_data_files)\n",
    "print(\"Test:\")\n",
    "test_min_num = print_classes(test_data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2da18f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "def adjust_classes(files, min_num):\n",
    "    tmp_files = []\n",
    "    for i in range(N_CLASSES):\n",
    "        filtered_files = []\n",
    "        for j in range(len(files)):\n",
    "            if len(filtered_files) == min_num:\n",
    "                break\n",
    "            label = float2int[video2label[files[j]]]\n",
    "            if label == i:\n",
    "                filtered_files.append(files[j])\n",
    "        tmp_files.extend(filtered_files)\n",
    "        \n",
    "    res_files = []\n",
    "    indices = list(range(len(tmp_files)))\n",
    "    random.seed(7)\n",
    "    random.shuffle(indices)\n",
    "    for idx in indices:\n",
    "        res_files.append(tmp_files[idx])\n",
    "    \n",
    "    return res_files\n",
    "\n",
    "train_data_files = adjust_classes(train_data_files, train_min_num)\n",
    "print(len(train_data_files))\n",
    "test_data_files = adjust_classes(test_data_files, test_min_num)\n",
    "print(len(test_data_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d20d9872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "0 40/80: 50.0%\n",
      "1 40/80: 50.0%\n",
      "Test:\n",
      "0 14/28: 50.0%\n",
      "1 14/28: 50.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train:\")\n",
    "print_classes(train_data_files)\n",
    "print(\"Test:\")\n",
    "print_classes(test_data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe303d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_or_read_file_names(pickle_name, train_data_files, test_data_files):\n",
    "    if os.path.isfile(pickle_name):\n",
    "        return load_features(pickle_name)\n",
    "    else:\n",
    "        save_features(pickle_name, [train_data_files, test_data_files])\n",
    "    return train_data_files, test_data_files\n",
    "\n",
    "train_data_files, test_data_files = write_or_read_file_names(\"ajusted_classes.pickle\", train_data_files, test_data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0281c6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7407407407407407\n"
     ]
    }
   ],
   "source": [
    "SPLIT_RATIO = len(train_data_files) / (len(train_data_files) + len(test_data_files))\n",
    "print(SPLIT_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83586e17",
   "metadata": {},
   "source": [
    "## Prepare custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcdddf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERS_DATASET = \"/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/\"\n",
    "\n",
    "DISTRACTED_DIR = os.path.join(USERS_DATASET, \"distracted/\")\n",
    "ENGAGED_DIR = os.path.join(USERS_DATASET, \"engaged/\")\n",
    "DISTRACTED_DIR_FACES = os.path.join(USERS_DATASET, \"distracted_faces/\")\n",
    "ENGAGED_DIR_FACES = os.path.join(USERS_DATASET, \"engaged_faces/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f22c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_frames(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    return int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "def get_video_frames(video_file, n_frames=1, frame_processor=None):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))//n_frames\n",
    "    for frame_count in range(n_frames,n_frames*(total_frames+1),n_frames):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES,frame_count)\n",
    "        ret, frame_bgr = cap.read()\n",
    "        if not ret:\n",
    "            cap.release()\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "        if frame_processor is not None:\n",
    "            frame = frame_processor(frame)\n",
    "        yield frame\n",
    "\n",
    "def preprocess(frame):\n",
    "    from functools import wraps\n",
    "    import sys\n",
    "    import io\n",
    "    def capture_output(func):\n",
    "        \"\"\"Wrapper to capture print output.\"\"\"\n",
    "\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            old_stdout = sys.stdout\n",
    "            new_stdout = io.StringIO()\n",
    "            sys.stdout = new_stdout\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            finally:\n",
    "                sys.stdout = old_stdout\n",
    "\n",
    "        return wrapper\n",
    "    \n",
    "    detector = MTCNN()\n",
    "    w_detect_face = capture_output(detector.detect_faces)\n",
    "    detected = w_detect_face(frame)\n",
    "    if len(detected) == 0:\n",
    "        return None\n",
    "    face = detected[0]\n",
    "    x1, y1, w, h = face['box'][0:4]\n",
    "    x2 = x1 + w\n",
    "    y2 = y1 + h\n",
    "    frame = frame[y1:y2, x1:x2, :]\n",
    "    return cv2.resize(frame, INPUT_SIZE)\n",
    "\n",
    "def process_videos(video_dir, result_dir):\n",
    "    for vid in os.listdir(video_dir):\n",
    "        vid_path = os.path.join(video_dir, vid)\n",
    "        out_path = os.path.join(result_dir, vid.split('.')[0])\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        else:\n",
    "            print(\"Skipping: \", vid)\n",
    "            continue\n",
    "        total_frames = get_total_frames(vid_path)\n",
    "        frames = get_video_frames(vid_path, frame_processor=preprocess)\n",
    "        for i, f in enumerate(frames):\n",
    "            print(\"[{}: {}/{}] \".format(vid, i, total_frames), end='\\r')\n",
    "            if f is None:\n",
    "                continue\n",
    "            cv2.imwrite(os.path.join(out_path, str(i) + '.png'), cv2.cvtColor(f, cv2.COLOR_RGB2BGR))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c708203b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDISTRACTED_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDISTRACTED_DIR_FACES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m process_videos(ENGAGED_DIR, ENGAGED_DIR_FACES)\n",
      "Cell \u001b[0;32mIn[19], line 53\u001b[0m, in \u001b[0;36mprocess_videos\u001b[0;34m(video_dir, result_dir)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_videos\u001b[39m(video_dir, result_dir):\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m vid \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     54\u001b[0m         vid_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(video_dir, vid)\n\u001b[1;32m     55\u001b[0m         out_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(result_dir, vid\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted/'"
     ]
    }
   ],
   "source": [
    "process_videos(DISTRACTED_DIR, DISTRACTED_DIR_FACES)\n",
    "process_videos(ENGAGED_DIR, ENGAGED_DIR_FACES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8cd59e",
   "metadata": {},
   "source": [
    "### Extract all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "231c04f4-17bc-456d-87c0-0fa60c45b3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n"
     ]
    }
   ],
   "source": [
    "path = '/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/engaged_faces/Egor_engaged_10/'\n",
    "print(len(os.listdir(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9799cf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir:  Yana_engaged_2\n",
      "Dir:  Katya 16 engaged\n",
      "Dir:  Den_engaged_5\n",
      "Dir:  Egor_engaged_4\n",
      "Dir:  Katya 1 engaged\n",
      "Dir:  Egor_engaged_13\n",
      "Dir:  Egor_engaged_3\n",
      "Dir:  Yana_engaged_10\n",
      "Dir:  Den_engaged_10\n",
      "Dir:  Kate_engaged_9\n",
      "Dir:  Den_engaged_7\n",
      "Dir:  Katya 12 engaged\n",
      "Dir:  Kate_engaged_6\n",
      "Dir:  Yana_engaged_5\n",
      "Dir:  Yana_engaged_6\n",
      "Dir:  Den_engaged_2\n",
      "Dir:  Kate_engaged_16\n",
      "Dir:  Kate_engaged_4\n",
      "Dir:  Katya 14 engaged\n",
      "Dir:  Egor_engaged_16\n",
      "Dir:  Den_engaged_4\n",
      "Dir:  Den_engaged_14\n",
      "Dir:  Katya 9 engaged\n",
      "Dir:  Kate_engaged_7\n",
      "Dir:  Katya 10 engaged\n",
      "Dir:  Yana_engaged_14\n",
      "Dir:  Kate_engaged_1\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/engaged_faces/Kate_engaged_1/0.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/engaged_faces/Kate_engaged_1/1.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/engaged_faces/Kate_engaged_1/2.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/engaged_faces/Kate_engaged_1/3.png\n",
      "Dir:  Yana_engaged_7\n",
      "Dir:  Yana_engaged_16\n",
      "Dir:  Egor_engaged_1\n",
      "Dir:  Den_engaged_11\n",
      "Dir:  Katya 3 engaged\n",
      "Dir:  Kate_engaged_14\n",
      "Dir:  Den_engaged_9\n",
      "Dir:  Katya 8 engaged\n",
      "Dir:  Den_engaged_16\n",
      "Dir:  Den_engaged_1\n",
      "Dir:  Egor_engaged_14\n",
      "Dir:  Den_engaged_12\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/engaged_faces/Den_engaged_12/140.png\n",
      "Dir:  Kate_engaged_15\n",
      "Dir:  Egor_engaged_2\n",
      "Dir:  Yana_engaged_13\n",
      "Dir:  Yana_engaged_1\n",
      "Dir:  Den_engaged_13\n",
      "Dir:  Den_engaged_8\n",
      "Dir:  Egor_engaged_6\n",
      "Dir:  Egor_engaged_5\n",
      "Dir:  Den_engaged_15\n",
      "Dir:  Den_engaged_3\n",
      "Dir:  Katya 15 engaged\n",
      "Dir:  Katya 5 engaged\n",
      "Dir:  Egor_engaged_9\n",
      "Dir:  Kate_engaged_8\n",
      "Dir:  Kate_engaged_11\n",
      "Dir:  Katya 13 engaged\n",
      "Dir:  Yana_engaged_15\n",
      "Dir:  Katya 6 engaged\n",
      "Dir:  Katya 2 engaged\n",
      "Dir:  Kate_engaged_3\n",
      "Dir:  Egor_engaged_7\n",
      "Dir:  Yana_engaged_4\n",
      "Dir:  Katya 7 engaged\n",
      "Dir:  Yana_engaged_9\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/engaged_faces/Yana_engaged_9/4.png\n",
      "Dir:  Kate_engaged_2\n",
      "Dir:  Yana_engaged_3\n",
      "Dir:  Egor_engaged_10\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/engaged_faces/Egor_engaged_10/144.png\n",
      "Dir:  Kate_engaged_5\n",
      "Dir:  Yana_engaged_11\n",
      "Dir:  Yana_engaged_8\n",
      "Dir:  Kate_engaged_12\n",
      "Dir:  Kate_engaged_13\n",
      "Dir:  Egor_engaged_11\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/engaged_faces/Egor_engaged_11/137.png\n",
      "Dir:  Egor_engaged_8\n",
      "Dir:  Kate_engaged_10\n",
      "Dir:  Den_engaged_6\n",
      "Dir:  Katya 4 engaged\n",
      "Dir:  Egor_engaged_15\n",
      "Dir:  Egor_engaged_12\n",
      "Dir:  Katya 11 engaged\n",
      "Dir:  Yana_engaged_12\n",
      "Dir:  Kate_distracted_3\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_3/8.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_3/9.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_3/10.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_3/11.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_3/12.png\n",
      "Dir:  Kate_distracted_13\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_13/169.png\n",
      "Dir:  Kate_distracted_1\n",
      "Dir:  Den_distracted_12\n",
      "Dir:  Katya 7 distracted\n",
      "Dir:  Egor_distracted_6\n",
      "Dir:  Kate_distracted_14\n",
      "Dir:  Yana_distracted_3\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/0.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/1.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/2.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/3.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/4.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/5.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/6.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/7.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/8.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/9.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/10.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/11.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/12.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/13.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/14.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/15.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/16.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/17.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/18.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/19.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/20.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/21.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/22.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/23.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/24.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/25.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/26.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/27.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/28.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/29.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/30.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/31.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/32.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/33.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/34.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/35.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/36.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/37.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/38.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/39.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/40.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/41.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/42.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/43.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/44.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/45.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/46.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/47.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/48.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/49.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/50.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/51.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/52.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/53.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/54.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/55.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_3/56.png\n",
      "Dir:  Katya 16 distracted\n",
      "Dir:  Kate_distracted_15\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_15/169.png\n",
      "Dir:  Egor_distracted_1\n",
      "Dir:  Den_distracted_6\n",
      "Dir:  Den_distracted_9\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_9/9.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_9/10.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_9/13.png\n",
      "Dir:  Kate_distracted_10\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_10/106.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_10/108.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_10/111.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_10/113.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_10/117.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_10/119.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_10/120.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_10/121.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_10/122.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_10/123.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_10/124.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_10/125.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_10/126.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_10/127.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_10/128.png\n",
      "Dir:  Egor_distracted_7\n",
      "Dir:  Egor_distracted_10\n",
      "Dir:  Egor_distracted_2\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Egor_distracted_2/42.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Egor_distracted_2/43.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Egor_distracted_2/44.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Egor_distracted_2/46.png\n",
      "Dir:  Egor_distracted_3\n",
      "Dir:  Katya 6 distracted\n",
      "Dir:  Katya 4 distracted\n",
      "Dir:  Yana_distracted_7\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/23.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/24.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/25.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/26.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/27.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/28.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/29.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/30.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/31.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/32.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/33.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/34.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/35.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/36.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/37.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/38.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/39.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/44.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/45.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/46.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/47.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/48.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/64.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/65.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/117.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/119.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/121.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/123.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/125.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_7/128.png\n",
      "Dir:  Egor_distracted_16\n",
      "Dir:  Yana_distracted_9\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_9/12.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_9/90.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_9/92.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_9/94.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_9/95.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_9/96.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_9/97.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_9/98.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_9/102.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_9/107.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_9/108.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_9/109.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_9/120.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_9/130.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_9/135.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_9/136.png\n",
      "Dir:  Yana_distracted_1\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_1/124.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_1/125.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_1/126.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_1/127.png\n",
      "Dir:  Egor_distracted_15\n",
      "Dir:  Yana_distracted_8\n",
      "Dir:  Den_distracted_11\n",
      "Dir:  Den_distracted_1\n",
      "Dir:  Kate_distracted_8\n",
      "Dir:  Egor_distracted_9\n",
      "Dir:  Yana_distracted_5\n",
      "Dir:  Egor_distracted_11\n",
      "Dir:  Den_distracted_13\n",
      "Dir:  Kate_distracted_4\n",
      "Dir:  Yana_distracted_16\n",
      "Dir:  Egor_distracted_14\n",
      "Dir:  Kate_distracted_16\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_16/14.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_16/16.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_16/126.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_16/127.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_16/128.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_16/132.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_16/135.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_16/136.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_16/138.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_16/139.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_16/140.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_16/141.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_16/142.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_16/146.png\n",
      "Dir:  Den_distracted_2\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/0.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/1.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/2.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/3.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/5.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/7.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/8.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/9.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/10.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/12.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/13.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/14.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/15.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/16.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/17.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/18.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/19.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/20.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/21.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/22.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/24.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/25.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/26.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/27.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/28.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/29.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/31.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/32.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/33.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/34.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/35.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/38.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/39.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/42.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/43.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/44.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/45.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/46.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/47.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/53.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/54.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/61.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/62.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/63.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/64.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_2/66.png\n",
      "Dir:  Yana_distracted_4\n",
      "Dir:  Kate_distracted_2\n",
      "Dir:  Egor_distracted_4\n",
      "Dir:  Yana_distracted_15\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_15/69.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_15/84.png\n",
      "Dir:  Yana_distracted_14\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/61.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/63.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/66.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/68.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/72.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/74.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/75.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/76.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/77.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/78.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/79.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/80.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/81.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/82.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/83.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/84.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/85.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/86.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/87.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/88.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/89.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/90.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/91.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/92.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/93.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/94.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/95.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/96.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/98.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/99.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_14/100.png\n",
      "Dir:  Katya 12 distracted\n",
      "Dir:  Yana_distracted_10\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/23.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/24.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/25.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/26.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/27.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/28.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/29.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/30.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/31.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/36.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/37.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/38.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/39.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/40.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/41.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/42.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/43.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/44.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/45.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/46.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/47.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/48.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/49.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/64.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/66.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/68.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/75.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/78.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/79.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/81.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/87.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_10/88.png\n",
      "Dir:  Kate_distracted_5\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/48.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/49.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/50.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/51.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/52.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/53.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/54.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/55.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/56.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/57.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/58.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/59.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/60.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/61.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/62.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/63.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/64.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/65.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/66.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/67.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/68.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/69.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/70.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/71.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/72.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/73.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/74.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/75.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/76.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/77.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_5/79.png\n",
      "Dir:  Den_distracted_7\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/60.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/63.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/65.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/67.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/68.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/69.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/70.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/71.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/72.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/73.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/74.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/75.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/76.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/77.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/78.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/81.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/83.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/87.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/88.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/89.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/90.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/91.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/92.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/93.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/94.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/95.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/96.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/97.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/98.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/99.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/100.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/101.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/102.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_7/103.png\n",
      "Dir:  Den_distracted_3\n",
      "Dir:  Yana_distracted_2\n",
      "Dir:  Katya 11 distracted\n",
      "Dir:  Kate_distracted_6\n",
      "Dir:  Katya 10 distracted\n",
      "Dir:  Den_distracted_16\n",
      "Dir:  Egor_distracted_8\n",
      "Dir:  Den_distracted_4\n",
      "Dir:  Yana_distracted_6\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_6/45.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_6/46.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_6/47.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_6/48.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_6/49.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_6/53.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_6/59.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_6/60.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_6/61.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_6/66.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_6/67.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_6/70.png\n",
      "Dir:  Yana_distracted_13\n",
      "Dir:  Katya 15 distracted\n",
      "Dir:  Katya 3 distracted\n",
      "Dir:  Den_distracted_15\n",
      "Dir:  Yana_distracted_11\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/53.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/54.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/59.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/61.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/62.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/63.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/65.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/66.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/67.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/70.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/71.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/72.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/74.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/75.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/76.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/77.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/78.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/79.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/80.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/81.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/88.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/89.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/90.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/91.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/92.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/93.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/94.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_11/95.png\n",
      "Dir:  Katya 9 distracted\n",
      "Dir:  Katya 1 distracted\n",
      "Dir:  Den_distracted_10\n",
      "Dir:  Katya 14 distracted\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Katya 14 distracted/65.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Katya 14 distracted/66.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Katya 14 distracted/67.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Katya 14 distracted/68.png\n",
      "Dir:  Egor_distracted_5\n",
      "Dir:  Kate_distracted_12\n",
      "Dir:  Katya 5 distracted\n",
      "Dir:  Katya 2 distracted\n",
      "Dir:  Katya 13 distracted\n",
      "Dir:  Egor_distracted_13\n",
      "Dir:  Den_distracted_5\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Den_distracted_5/137.png\n",
      "Dir:  Den_distracted_14\n",
      "Dir:  Katya 8 distracted\n",
      "Dir:  Kate_distracted_11\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_11/55.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_11/86.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_11/88.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_11/89.png\n",
      "Dir:  Den_distracted_8\n",
      "Dir:  Kate_distracted_9\n",
      "Dir:  Yana_distracted_12\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/10.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/11.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/12.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/13.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/14.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/15.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/16.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/17.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/18.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/19.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/20.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/21.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/22.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/23.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/24.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/25.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/26.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/27.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/28.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/29.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/30.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/31.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/32.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/33.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/34.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/35.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/36.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/37.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/38.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/39.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/40.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/41.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/42.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/43.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/44.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/45.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/46.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/47.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Yana_distracted_12/49.png\n",
      "Dir:  Kate_distracted_7\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/19.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/20.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/21.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/22.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/23.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/24.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/25.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/26.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/27.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/28.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/29.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/30.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/31.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/32.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/33.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/34.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/35.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/36.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/37.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/38.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/39.png\n",
      "/home/echuraev/Workspace/HSE/datasets/Engagement_dataset/distracted_faces/Kate_distracted_7/40.png\n",
      "Dir:  Egor_distracted_12\n"
     ]
    }
   ],
   "source": [
    "def get_frames_paths(directory):\n",
    "    video2frames = {}\n",
    "    for video_dir in os.listdir(directory):\n",
    "        print(\"Dir: \", video_dir)\n",
    "        path = os.path.join(directory, video_dir)\n",
    "        total_files = len(os.listdir(path))\n",
    "        video2frames[video_dir] = []\n",
    "        i = 0\n",
    "        while i < total_files:\n",
    "            file_name = str(i) + \".png\"\n",
    "            file_path = os.path.join(path, file_name)\n",
    "            i += 1\n",
    "            if not os.path.exists(file_path):\n",
    "                print(file_path)\n",
    "                continue\n",
    "            video2frames[video_dir].append(file_path)\n",
    "    return video2frames\n",
    "\n",
    "engaged2frames = get_frames_paths(ENGAGED_DIR_FACES)\n",
    "distracted2frames = get_frames_paths(DISTRACTED_DIR_FACES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "786128a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "def extract_features_my_ds(vid2frame, label):\n",
    "    vid2features = {}\n",
    "    #import re\n",
    "    for j, k in enumerate(vid2frame.keys()):\n",
    "        print('[{}/{}]'.format(j+1, len(vid2frame)), end=\"\\r\")\n",
    "        #actor_name = re.split(\" |_\", k)[0]\n",
    "        vid2features[k] = {}\n",
    "        imgs = []\n",
    "        sequence = []\n",
    "        for i, vid in enumerate(vid2frame[k]):\n",
    "            img_file = vid\n",
    "            if torch_model:\n",
    "                image = Image.open(img_file)\n",
    "                img_tensor = test_transforms(image)\n",
    "                imgs.append(img_tensor)\n",
    "            else:\n",
    "                image = cv2.imread(img_file)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                imgs.append(image)\n",
    "            \n",
    "            if len(imgs) >= BATCH_SIZE:\n",
    "                if torch_model:\n",
    "                    features = cnn_model(torch.stack(imgs, dim=0).to(device))\n",
    "                    features = features.data.cpu().numpy()\n",
    "                else:\n",
    "                    features = cnn_model.predict(np.array(imgs), verbose=0)\n",
    "                if len(sequence) == 0:\n",
    "                    sequence = features\n",
    "                else:\n",
    "                    sequence = np.concatenate((sequence, features),axis=0)\n",
    "                imgs = []  \n",
    "        if len(imgs) > 0:        \n",
    "            if torch_model:\n",
    "                features = cnn_model(torch.stack(imgs, dim=0).to(device))\n",
    "                features = features.data.cpu().numpy()\n",
    "            else:\n",
    "                features = cnn_model.predict(np.array(imgs), verbose=0)\n",
    "            if len(sequence) == 0:\n",
    "                sequence = features\n",
    "            else:\n",
    "                sequence = np.concatenate((sequence, features),axis=0)\n",
    "        \n",
    "        vid2features[k]['features'] = sequence\n",
    "        vid2features[k]['label'] = label \n",
    "    print()\n",
    "    return vid2features\n",
    "\n",
    "def extract_or_read_features_my_ds(file_pickle, vid2frame = None, label: int = -1):\n",
    "    if os.path.isfile(file_pickle):\n",
    "        vid2features = load_features(file_pickle)\n",
    "    elif vid2frame is None or label < 0:\n",
    "        raise Exception(\"Cannot read file: \", file_pickle)\n",
    "    else:\n",
    "        vid2features = extract_features_my_ds(vid2frame, label)\n",
    "        save_features(file_pickle, vid2features)\n",
    "    return vid2features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbede546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  enet_b0_8_best_afew.pt\n",
      "Engaged:\n",
      "[80/80]\n",
      "Distracted:\n",
      "[80/80]\n",
      "Model:  mobilenet_7.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 08:47:04.065703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 32509 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:11:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Engaged:\n",
      "[1/80]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 08:47:05.473873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80/80]\n",
      "Distracted:\n",
      "[80/80]\n"
     ]
    }
   ],
   "source": [
    "for base_model_key in model_urls.keys():\n",
    "    print(\"Model: \", base_model_key)\n",
    "    download_model(model_urls[base_model_key], base_model_key)\n",
    "    base_model_name = './' + base_model_key\n",
    "    torch_model = True if base_model_name[base_model_name.rfind('.') + 1:] == 'pt' else False\n",
    "    if torch_model:\n",
    "        BATCH_SIZE = 64\n",
    "    else:\n",
    "        BATCH_SIZE = 128\n",
    "        \n",
    "    if base_model_key in ('enet_b2_7.pt', 'enet_b2_8.pt'):\n",
    "        cnn_model = torch.load(base_model_name)\n",
    "        cnn_model = cnn_model.to(device)\n",
    "        cnn_model.eval()\n",
    "\n",
    "        #del cnn_model.classifier\n",
    "        last_layer=torch.nn.Sequential(cnn_model.classifier,torch.nn.Softmax(dim=0))\n",
    "        if True:\n",
    "            inp = torch.randn(1408).to(device)\n",
    "            f=last_layer.forward(inp)\n",
    "            #print(f.shape,f,f.sum(axis=1))\n",
    "        cnn_model.classifier=torch.nn.Identity()\n",
    "        #print(cnn_model)\n",
    "    elif base_model_key in ('enet_b0_8_va_mtl.pt'):\n",
    "        cnn_model = torch.load(base_model_name)\n",
    "        cnn_model = cnn_model.to(device)\n",
    "        cnn_model.eval()\n",
    "\n",
    "        #del cnn_model.classifier\n",
    "        last_layer=torch.nn.Sequential(cnn_model.classifier,torch.nn.Softmax(dim=1))\n",
    "        if True:\n",
    "            inp = torch.randn(20, 1280).to(device)\n",
    "            f=last_layer.forward(inp)\n",
    "            #print(f.shape,f,f.sum(axis=1))\n",
    "        cnn_model.classifier=torch.nn.Identity()\n",
    "        #print(cnn_model)\n",
    "    elif torch_model:\n",
    "        cnn_model = torch.load(base_model_name)\n",
    "        cnn_model = cnn_model.to(device)\n",
    "        cnn_model.eval()\n",
    "\n",
    "        #del cnn_model.classifier\n",
    "        last_layer=torch.nn.Sequential(cnn_model.classifier[0],torch.nn.Softmax(dim=1))\n",
    "        if True:\n",
    "            inp = torch.randn(20, 1280).to(device)\n",
    "            f=last_layer.forward(inp)\n",
    "            #print(f.shape,f,f.sum(axis=1))\n",
    "        cnn_model.classifier=torch.nn.Identity()\n",
    "    else:\n",
    "        base_model = load_model(base_model_name)\n",
    "        layer_out = base_model.get_layer('global_pooling')\n",
    "        cnn_model = Model(base_model.input,layer_out.output)\n",
    "\n",
    "        #cnn_model.summary()\n",
    "    \n",
    "    engaged_pickle = USERS_DATASET + 'engaged_features_{}.experiments.pickle'.format(base_model_key)\n",
    "    distracted_pickle = USERS_DATASET + 'distracted_features_{}.experiments.pickle'.format(base_model_key)\n",
    "    print(\"Engaged:\")\n",
    "    _ = extract_or_read_features_my_ds(engaged_pickle, engaged2frames, 1)\n",
    "    print(\"Distracted:\")\n",
    "    _ = extract_or_read_features_my_ds(distracted_pickle, distracted2frames, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b96db4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Extract all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d66e3c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  enet_b0_8_va_mtl.pt\n",
      "Create train dataset\n",
      "Create test dataset\n",
      "[28/28]\r"
     ]
    }
   ],
   "source": [
    "for base_model_key in model_urls.keys():\n",
    "    print(\"Model: \", base_model_key)\n",
    "    download_model(model_urls[base_model_key], base_model_key)\n",
    "    base_model_name = './' + base_model_key\n",
    "    torch_model = True if base_model_name[base_model_name.rfind('.') + 1:] == 'pt' else False\n",
    "    if torch_model:\n",
    "        BATCH_SIZE = 64\n",
    "    else:\n",
    "        BATCH_SIZE = 128\n",
    "        \n",
    "    if base_model_key in ('enet_b2_7.pt', 'enet_b2_8.pt'):\n",
    "        cnn_model = torch.load(base_model_name)\n",
    "        cnn_model = cnn_model.to(device)\n",
    "        cnn_model.eval()\n",
    "\n",
    "        #del cnn_model.classifier\n",
    "        last_layer=torch.nn.Sequential(cnn_model.classifier,torch.nn.Softmax(dim=0))\n",
    "        if True:\n",
    "            inp = torch.randn(1408).to(device)\n",
    "            f=last_layer.forward(inp)\n",
    "            #print(f.shape,f,f.sum(axis=1))\n",
    "        cnn_model.classifier=torch.nn.Identity()\n",
    "        #print(cnn_model)\n",
    "    elif base_model_key in ('enet_b0_8_va_mtl.pt'):\n",
    "        cnn_model = torch.load(base_model_name)\n",
    "        cnn_model = cnn_model.to(device)\n",
    "        cnn_model.eval()\n",
    "\n",
    "        #del cnn_model.classifier\n",
    "        last_layer=torch.nn.Sequential(cnn_model.classifier,torch.nn.Softmax(dim=1))\n",
    "        if True:\n",
    "            inp = torch.randn(20, 1280).to(device)\n",
    "            f=last_layer.forward(inp)\n",
    "            #print(f.shape,f,f.sum(axis=1))\n",
    "        cnn_model.classifier=torch.nn.Identity()\n",
    "        #print(cnn_model)\n",
    "    elif torch_model:\n",
    "        cnn_model = torch.load(base_model_name)\n",
    "        cnn_model = cnn_model.to(device)\n",
    "        cnn_model.eval()\n",
    "\n",
    "        #del cnn_model.classifier\n",
    "        last_layer=torch.nn.Sequential(cnn_model.classifier[0],torch.nn.Softmax(dim=1))\n",
    "        if True:\n",
    "            inp = torch.randn(20, 1280).to(device)\n",
    "            f=last_layer.forward(inp)\n",
    "            #print(f.shape,f,f.sum(axis=1))\n",
    "        cnn_model.classifier=torch.nn.Identity()\n",
    "    else:\n",
    "        base_model = load_model(base_model_name)\n",
    "        layer_out = base_model.get_layer('global_pooling')\n",
    "        cnn_model = Model(base_model.input,layer_out.output)\n",
    "\n",
    "        #cnn_model.summary()\n",
    "    \n",
    "    train_pickle = FEATURES_DIR + 'train_features_engagement_{}.pickle'.format(base_model_key)\n",
    "    test_pickle = FEATURES_DIR + 'test_features_engagement_{}.pickle'.format(base_model_key)\n",
    "    train_features, train_data_files, test_features, test_data_files = extract_or_read_features(train_pickle, test_pickle, train_data_files, test_data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42320a6-4bd5-4a15-b2aa-afae38092dab",
   "metadata": {},
   "source": [
    "## Read features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6edb58b-c5fe-453c-9165-a835424fff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_key = 'enet_b0_8_best_afew.pt'\n",
    "USERS_FEATURES = USERS_DATASET + 'features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07fdfbab-50ed-4fcb-a4c3-4e405fd1748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "engaged_pickle = USERS_DATASET + 'engaged_features_{}.extractor.pickle'.format(base_model_key)\n",
    "eng_vid2features = load_features(engaged_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99c291ab-b403-4b78-bd51-3c28bc2fc34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "engaged_pickle_ref = USERS_FEATURES + 'engaged_features_{}.good.pickle'.format(base_model_key)\n",
    "eng_vid2features_ref = load_features(engaged_pickle_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f88ba1b-6141-4a2c-9d0c-209e462a9308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'features': array([[-0.17457296, -0.0838912 ,  0.0130755 , ..., -0.13524437,\n",
      "        -0.19775935,  0.08375152],\n",
      "       [-0.17530565, -0.09148189, -0.00201505, ..., -0.1188405 ,\n",
      "        -0.20162667,  0.13017893],\n",
      "       [-0.17212895, -0.09340156, -0.00343952, ..., -0.1270643 ,\n",
      "        -0.2020545 ,  0.14086863],\n",
      "       ...,\n",
      "       [-0.15957235, -0.10684593, -0.04282501, ..., -0.12603702,\n",
      "        -0.05989177,  0.14404123],\n",
      "       [-0.12564267, -0.12438735, -0.04556637, ..., -0.1355438 ,\n",
      "        -0.12425207,  0.16872056],\n",
      "       [-0.09237796, -0.11252303, -0.07996154, ..., -0.17252916,\n",
      "        -0.15970832,  0.07588568]], dtype=float32), 'label': 1}\n",
      "{'features': array([[-0.17462769, -0.08404563,  0.01284902, ..., -0.13499106,\n",
      "        -0.19764735,  0.08371742],\n",
      "       [-0.17534773, -0.0915754 , -0.00221882, ..., -0.11862936,\n",
      "        -0.20157237,  0.13044624],\n",
      "       [-0.17213255, -0.09351983, -0.00365608, ..., -0.12657824,\n",
      "        -0.2019372 ,  0.1411498 ],\n",
      "       ...,\n",
      "       [-0.15943566, -0.10684813, -0.04302968, ..., -0.12604704,\n",
      "        -0.05978133,  0.14417414],\n",
      "       [-0.12570632, -0.1244224 , -0.04589483, ..., -0.13566375,\n",
      "        -0.12425329,  0.16908844],\n",
      "       [-0.09230021, -0.11249769, -0.08003167, ..., -0.172423  ,\n",
      "        -0.15969947,  0.07581984]], dtype=float32), 'label': 1}\n",
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n",
      "[[ 5.47319651e-05  1.54428184e-04  2.26487406e-04 ... -2.53304839e-04\n",
      "  -1.11997128e-04  3.41013074e-05]\n",
      " [ 4.20808792e-05  9.35047865e-05  2.03777337e-04 ... -2.11142004e-04\n",
      "  -5.42998314e-05 -2.67311931e-04]\n",
      " [ 3.60608101e-06  1.18270516e-04  2.16564396e-04 ... -4.86060977e-04\n",
      "  -1.17301941e-04 -2.81170011e-04]\n",
      " ...\n",
      " [-1.36688352e-04  2.19792128e-06  2.04663724e-04 ...  1.00284815e-05\n",
      "  -1.10439956e-04 -1.32918358e-04]\n",
      " [ 6.36428595e-05  3.50549817e-05  3.28458846e-04 ...  1.19954348e-04\n",
      "   1.22189522e-06 -3.67879868e-04]\n",
      " [-7.77542591e-05 -2.53394246e-05  7.01323152e-05 ... -1.06155872e-04\n",
      "  -8.85128975e-06  6.58407807e-05]]\n"
     ]
    }
   ],
   "source": [
    "print(eng_vid2features['Yana_engaged_2'])\n",
    "print(eng_vid2features_ref['Yana_engaged_2'])\n",
    "print(eng_vid2features['Yana_engaged_2']['features'] == eng_vid2features_ref['Yana_engaged_2']['features'])\n",
    "print(eng_vid2features['Yana_engaged_2']['features'] - eng_vid2features_ref['Yana_engaged_2']['features'])\n",
    "np.testing.assert_allclose(eng_vid2features['Yana_engaged_2']['features'], eng_vid2features_ref['Yana_engaged_2']['features'], rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf0226-fa5b-427a-bfab-fe8ab51487b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "366.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
